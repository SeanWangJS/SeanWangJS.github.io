---
title: 函数空间优化
---

通用的学习模型可以被表示成下面三个部分的组合

1) 产生器，从一个未知但固定的概率分布函数 \(F(x)\) 中独立同分布地产生向量 \(x \in R^n\)。
2) 训练器，对于每一个 \(x\)，从一个未知但固定的条件概率分布 \(F(y\mid x)\) 中独立同分布地产生值 \(y\)。
3) 学习机器，根据上面产生的集合 \(\{(x_i, y_i)\}\)，从函数集 \(f(x, P), P\in \Lambda\) 中寻找最佳的参数 \(P^*\)，使得对于任意 \(x\)， \(\bar y = f(x, P)\) 能够最好的逼近训练器的响应 \(y\)。

通俗的讲，产生器和训练器共同生成了我们通常称为训练集的数据，所以也可以看作是从联合分布 \(F(x, y)\) 中独立同分布地抽取的，而我们的训练过程则是寻找最佳参数 \(P^*\) 的过程。如果我们在每个样本上定义损失函数

\[loss = L(y, f(x, P))\]

并且假设联合分布 \(F(x, y)\) 的联合概率密度函数为 \(p(x, y)\)，则总体期望损失为

\[
    R = \iint p(x, y) L(y, f(x, P)) \mathrm{d} x\mathrm{d}y
    \]

可见，\(R\) 可以看作是关于 \(P\) 的函数 \(R(P)\)，也可以看作是关于 \(f(x, P)\) 的泛函 \(R(f)\)，从后者的角度来看，\(R\)又被称为风险泛函。学习的目标便是寻找最佳的函数 \(f\) 或 参数\(P\) 使得总体期望损失最小化。

虽然 \(R\) 以积分的形式给出，但实际上我们能获取的训练数据都是离散的点，所以 \(R\) 也能被写成求和的形式

\[
    R = \sum_{i=1}^N w_i L(y_i, f(x_i, P))
    \]

其中 \(w_i\) 是每个点所占的权重，

#### 参数空间优化

当 \(R\) 被看作是关于 \(P\) 的函数时，求解最优参数可以表述成 

\[
    P^* = \arg \min_P R(P)
    \]

使用迭代方法，每一次更新都是在修正参数 \(P\)，于是最终结果就是初始值与一系列中间修正值之和

\[
    P^* = \sum p_i
    \]

其中 \(p_0\) 是选定的初始参数。对于具体的梯度下降法来说，每一步的参数增量都是目标函数值关于参数的梯度再乘以一个步长

\[
    p_i = - \rho_i \frac {\partial R(P)}{\partial P} \mid_{P=p_{i-1}} ,\quad i > 0
    \]

其中的步长 \(\rho_i\) 又被称为学习速率，它可以是一个固定值，但为了更好地使目标函数收敛，一般会使用线性搜索方法寻找最优步长，线性搜索的思想很直观，就是使本次更新后的目标函数关于步长取最小值，即

\[
    \rho_i = \arg\min_{\rho} R\left(p_{i-1}-\rho \frac {\partial R(P)}{\partial P} \mid_{P=p_{i-1}}\right) 
    \]

#### 函数空间优化

如果 \(R\) 被看作是关于 \(f\) 的泛函，那么优化过程便是在函数空间中寻找最佳函数 \(f\) 使得 \(R(f)\) 最小化。