---
title: 朴素贝叶斯分类器
tags: 机器学习
---

在分类问题中，需要根据给定的特征向量 \(X = (F_1, F_2,..,F_n)\) 来预测样本所属的类别 \(C\)，也就是说 \(C\) 的取值依赖于 \(X\)，从概率的角度来看，就是说

\[
  p(C\mid X)
  \]

如果我们知道 \(p(C\mid X)\) 的具体形式，那么能通过计算 \(p(C\mid X)\) 在 \(C\) 取各类别时的值来判断 \(C\) 最可能的类别，即

\[
  c = \argmax_C p(C\mid X)
  \]

利用贝叶斯公式，可以将其变换为

\[
  p(C\mid X) = \frac {p(X \mid C) p(C)}{p(X)}
  \]

其中 \(p(C)\) 是类先验概率，它的值可以从样本的分布来估计，\(p(X)\) 与类变量无关，因此它的值不影响 \(C\) 取各种类别的概率，可以看作常数。现在只剩下类条件概率 \(p(X\mid C)\) 需要计算，假如变量 \(X\) 只有一个特征，那么我们可以将样本按类别分组，再分别对每组的 \(p(X\mid C=c_k)\) 进行参数估计，从而解决了问题，但是如果 \(X\) 有多个特征，那么就不一定能直接估计出 \(p(X \mid C=c_k)\) 了。为了简化问题，朴素贝叶斯方法假设 \(X\) 的各个特征间相互独立，也就是说

\[
  p(X\mid C) = \prod_{i=1}^n p(F_i\mid C)
  \]

于是现在，只需要估计出对于每个\(i\) 的 \(p(F_i\mid C =c_k)\) ，就可以计算得到 \(p(X\mid C)\)，从而解决了计算 \(p(C\mid X)\) 的问题，于是

\[
  c = \argmax_C \frac 1 {p(X)} p(C)\prod_{i=1}^n p(F_i\mid C)
  \]

朴素贝叶斯分类器的原理就是这样，还是很容易理解的，下面我们讨论它的计算过程。假设给定样本集 
\[
  S = \{(X_i, C_j) \mid i = 1,,m, j = 1,,k\}
  \]

即样本数量为 \(m\)，类别数量为 \(k\)。首先我们来估计先验概率，假如在样本中，属于类 \(c_j\) 的样本数量为 \(|c_j|\)，那么

\[
  p(C = c_j) = \frac {|c_j|} m
  \]

上式相当简单直接，但如果样本中某个类别没有出现，将导致它的预测概率等于 0，这未免过于武断，因此实际应用中需要引入拉普拉斯平滑方法进行修正

\[
  p(C=c_j) = \frac {|c_j| + \lambda} {m + k \lambda}
  \]

其中 \(\lambda\) 常常取 1。这样就体现了如果某个类别没有出现在样本集合中，则它的先验概率便很小的状态，通俗的讲，就是有内味儿了。下一步需要估计的是类条件概率 \(p(X\mid C)\)，也就是每一个 \(p(F_i \mid C)\)，这时需要分两种情况进行讨论。

首先，当 \(F_i\) 是离散特征的时候，对它的估计类似于对 \(p(C)\) 的估计，设 \(F_i\) 的取值空间为 \(\{f_{is} \mid s = 0,,,N_i\}\)，再设 \(|F_{is}^j|\) 表示属于类别 \(c_j\) 的样本中 \(F_i=f_{is}\) 的数量，那么

\[
  p(F_i = f_{is}\mid C = c_j) = \frac {|F_{is}^j|} m
  \]

基于同样的理由，我们引入拉普拉斯平滑，重写成

\[
  p(F_i = f_{is} \mid C = c_j) = \frac {|F_{is}^j| + \lambda} {m + |F_{is}|\lambda}
  \]

其中 \(|F_{is}|\) 表示 \(F_i = f_{is}\) 的样本总数。

而如果 \(F_i\) 是连续变量，那么我们可以假设对于每个类别 \(c_j\)，\(p(F_i \mid C = c_j)\) 服从正太分布，即 

\[
  p(F_i = f \mid C = c_j) = \frac 1 {\sqrt {2\pi}\sigma_{ij}}\exp \left(-\frac {(f - \mu_{ij})^2}{2\sigma_{ij}}\right)
  \]

其中 \(\sigma_{ij}, \mu_{ij}\) 可以通过参数估计得到，它们分别是属于 \(c_j\) 的样本组中特征 \(F_i\) 的方差和均值。当然，说 \(F_i\) 服从正太分布只是先验假设，实际情况中它有可能属于其他分布，这时只需要调整它的参数估计方法即可。

以上内容，我们讲解了朴素贝叶斯分类器的原理和具体实现方法，其实就是利用朴素贝叶斯假设将条件概率 \(p(C\mid X)\)分解成了类先验概率和多个特征的类条件概率之积，然后使用最基本的参数估计方法来估计这些概率密度函数，最后通过计算各类别的条件概率，寻找使得概率最大的类别作为最终的预测值。

