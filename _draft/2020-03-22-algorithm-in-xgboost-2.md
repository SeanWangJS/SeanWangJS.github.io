---
title: XGBoost 算法原理详解(2)——近似贪心优化算法
---

#### 动机

使用精确贪心算法的问题在于每一个样本都会被作为候选分割点来计算一次损失增益，这带来了高昂的时间代价，为了解决这一问题，一个很直接的想法就是使用训练集 \(\mathcal{X}\) 中的少部分样本来作为候选分割点，这里假设为 \(S = \lbrace s_1, s_2, s_3 ,..., s_l\rbrace ,s_i \in\mathcal{X}\)。通过和精确贪心优化类似的算法框架，下面给出算法过程

1. 首先设分裂前的节点上的样本集合为 \(I\)，样本特征维度等于 \(m\)；
2. 初始化：损失增益变量 \(gain = 0\)，最佳分裂特征 \(d = 1\)，最佳分裂点 \(z = \min(\mathbf{x}_d)\)，这里的 \(\mathbf{x}_d\) 是所有候选样本 \(S\) 第 \(d\) 个特征组成的向量；
3. 计算 
\[
    G = \sum_{i \in I} g_i,\quad H = \sum_{i \in I}h_i
    \]
4. 对于 \(k = 1...m\)：
  a. 假设在第 \(k\) 个特征上分裂，为了确定在哪个值上分裂，先对第 \(k\) 个特征上的所有候选值 \(\{s_{jk}\}_{j=1}^l\) 进行排序，得到排序后的向量 \(\{\bar{s}_{jk}\}_{j=1}^l\)；
  b. 分组计算统计量
  \[
  G_{jk} = \sum_{i\in \lbrace i \mid s_{ik} < x_{jk} \le s_{{i+1},k} \rbrace} g_i ,\quad H_{jk} = \sum_{i\in \lbrace i \mid s_{jk} < x_{ik} \le s_{{j+1},k} \rbrace} h_i 
  \]
  c. 假设分裂后的两部分样本分别为 \(I_L\) 和 \(I_R\)，初始化 \(G_L = 0, H_L = 0\)，对于 \(j = 1... l\)：
  (1). 计算 
    \[
        G_L \leftarrow G_L + G_{jk}, H_L \leftarrow H_L + H_{jk}
    \]

    (2). 根据 \(G\) 的定义，得到 
    \[
        G_R \leftarrow G - G_L, H_R \leftarrow H - H_L
        \]
    
    (3). 计算损失增益
    \[
        \Delta L =  \frac {G_L^2}{H_L + \lambda} +\frac {G_R^2}{H_R + \lambda} -\frac{G^2}{H+\lambda}- \gamma 
        \]
    
    (4). 当 \(\Delta L > gain\) 时，更新 \(gain = \Delta L, d = k, z = \bar{s}_{jk}\)；

可以看到，与前一篇所描述的精确贪心算法唯一不同的地方在于计算最佳分裂点时只用到了样本集的一个子集，也就是所谓的候选分割点。

很明显，如果候选分割点集合包含理论上的最优分裂值或者与最优分裂点相近的值，那么上述近似算法将得到与精确算法相近的结果，否则将会产生一定的误差，所以现在的核心问题便是如何从全体样本中选出最有代表性的候选样本。

