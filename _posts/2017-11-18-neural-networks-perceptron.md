---
layout: default
---

## 神经网络：单层感知器

感知器这个名称听起来就很诡异，再加一些教材并不想好好说话，就更加令人费解这一个概念。其实所谓的感知器就是一系列分类面，而单层感知器则不过是二维面上的一条直线，或者高维空间的超平面，它具有统一的形式

\[
f(x_1,x_2,...x_m) = \sum_{i = 1}^m w_i x_i + b
\]

如果令 $x_0=1, \, w_0 = b$ ，那么就可以将上式写成更简单的形式

\[
f(\mathbf{x}) = \mathbf{w}^T \mathbf{x}
\]

其中 $\mathbf{w} = [w_1 \quad w_2 \quad ... w_m], \mathbf{x} = [x_1 \quad x_2 \quad ... x_m]$

这个函数的作用是，将特征向量 $\mathbf{x}$ 映射到一个值上，然后再将这个值应用于激活函数得到最终的结果，整个过程如下图所示

![](/resources/2017-11-18-neural-networks-perceptron/perceptron.png)

激活函数，比如阶跃函数，在当 $f(x)$ 大于等于 0 的时候输出 1，否则输出 0，这就完成了分类。接下来我们的任务是根据训练数据来求解权重向量 $\mathbf{w}$，假设我们有训练样本集 $S = \{(\mathbf{x}^{(i)}, y_i)|\,\, i \in \{1,2,3,...n\}\}$。这里的 $y_i$ 是第 i 个样本的观察结果，具有值 0 或 1，表示正负样本。如果我们有一个完美的解决方案（即非常合适的 $g$ 与 $\mathbf{w}$），使得每一个样本特征都能映射到正确的类别，即

\[
g^{-1}\left[\begin{matrix}
y_1\\y_2\\...\\y_n
\end{matrix}\right]=\mathbf{w}^T
\left[\begin{matrix}
\mathbf{x}^{(1)}\\
\mathbf{x}^{(2)}\\...\\
\mathbf{x}^{(n)}
\end{matrix}\right]
\]

然后再定义

\[
Y = [y_1\quad y_2... \quad y_n]^T\\
X = [\mathbf{x}^{(1)T}\quad \mathbf{x}^{(2)T}... \quad \mathbf{x}^{(n)T}]^T
\]

于是可以得到方程

\[
g^{-1}Y = \mathbf{w}^T X
\]

这里的未知量只有 $\mathbf{w}$，于是从某种意义上，我们需要求解一个更具内涵的一元一次方程。当然这只是从形式上的类似，实际应用到的求解方法应该复杂得多。

为了求解权重向量 $\mathbf{w}$，我们可以考虑把问题的形式改变一下。假设有一个初始的解 $\mathbf{w}$，将其带入激活函数，得到一组分类结果 $\{\hat{y}_i|\,\,i\in\{1,2,,,n\}\}$，对于每个结果，其值可能会和观察值不一样，于是定义累计误差来衡量这种不确定性

\[
e = \sum_{i=1}^n (\mathbf{w}^T \mathbf{x}^{(i)} - y_i)^2
\]

显然 $e$ 是与 $\mathbf{w}$ 有关的函数。并且 $e$ 是越小越好，因为这意味着有更多的训练样本得到了正确的分类。于是求解 $\mathbf{w}$ 的问题就转化成了求解最优化问题

\[
\min_{\mathbf{w}} \quad e
\]

这是一个相当简单的无约束凸优化问题，可以使用梯度下降法来求解。





end

end

end
