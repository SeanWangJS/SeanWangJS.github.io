---
title: 为什么说 Network In Network 等价于 1x1 卷积？
tags: 卷积神经网络
---

下图描述了一个最简单的卷积运算过程，卷积核在图片的空间维度上不断滑动，每停留一次就与其覆盖的 patch 做 element-wise 相乘，然后再求和作为输出特征上的一个元素。

![](/resources/2022-07-01-nin//nin_conv.png)

而如果输入和输出都是多通道的，则卷积核的数量等于前两者通道数量之积，用图形来表示的话如下所示

![](/resources/2022-07-01-nin//nin_conv-multi-channel.png)

特化到我们当前讨论的 `1x1` 卷积，则上图可以简化一下

![](/resources/2022-07-01-nin//nin_1x1-conv.png)

接下来，我们把每次卷积涉及到的元素单独提取出来

![](/resources/2022-07-01-nin//nin_1x1-conv-simple.png)

然后再调整一下排列顺序

![](/resources/2022-07-01-nin//nin_full-connect.png)

这样就可以看出来，`1x1` 卷积不过就是一种全连接层，它的输入和输出都是特征图每个空间位置在 channel 方向上的元素值组成的向量，卷积核则是该全连接层的权重参数。

Network In Network 是一种神经网络结构，论文作者指出，普通卷积核是针对于卷积块的广义线性模型，它无法提取图像更高层次的抽象信息，所以如果能找一个非线性能力更强的函数来代替卷积核，则可以增强局部模型的抽象能力。事实上，作者选择了多层感知机来代替卷积核，两者的区别如下图所示，可以看到，改进后的多层感知机卷积层把卷积块的所有元素按全连接的方式输入到感知机中。

![](/resources/2022-07-01-nin//nin_compare.png)

若把卷积块中的所有元素一维展开，则用下图可以更详细的展示计算细节

![](/resources/2022-07-01-nin//nin_mlpconv.png)

现在，让我们把上述 mlpconv 层简化一下，假设输入通道数为 $c_i$，输出通道为 $c_o$，patch 尺寸为 `1x1`，感知机改成单层的，则简化后的 mlpconv 如下所示
   
![](/resources/2022-07-01-nin//nin_1x1-full-connect.png)

这个结构与我们前面分析的 `1x1` 卷积的全连接形式一模一样，所以我们可以得出结论，如果 Network In Network 的卷积块大小为 `1x1`，且感知机为单层，则其结构等价于 `1x1` 卷积。